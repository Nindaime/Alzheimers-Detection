{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55355577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Disable GPU\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Disable GPU\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9c15e",
   "metadata": {},
   "source": [
    "## Import Relevant Libraries for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d28e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Concatenate, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import mplcursors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bebae",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2506b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Subject ID', 'MRI ID', 'Group', 'Visit', 'MR Delay', 'M/F', 'Hand',\n",
      "       'Age', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "372it [00:01, 217.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load CSV data\n",
    "csv_path = r'D:\\DATASET\\Dimentia\\dementia_dataset.csv'\n",
    "data = pd.read_csv(csv_path)\n",
    "data = data.fillna(0)\n",
    "\n",
    "print(data.columns)\n",
    "for column in data.drop(columns=['Group', 'Subject ID', 'MRI ID']).columns:\n",
    "        data[column] = data[column].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db80ab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features before feature selection (372, 11)\n",
      "ExtraTreesClassifier()\n"
     ]
    }
   ],
   "source": [
    "def generate_data(df, extra_trees_classifier):\n",
    "    X = df[['Age', 'EDUC', 'Visit', 'MR Delay', 'Hand', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF']]\n",
    "    print(f'Number of Features before feature selection {X.shape}')\n",
    "    y = df['Group'] # set the y to the dependent output variable\n",
    "    model = extra_trees_classifier()\n",
    "    model.fit(X,y)\n",
    "    print(model)\n",
    "    return X, y, model\n",
    "    \n",
    "X, y, model = generate_data(data, ExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defc2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(y):\n",
    "    onehot = pd.get_dummies(y)\n",
    "    output = onehot.to_numpy()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b7a601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 11)\n",
      "   Age  EDUC  Visit  MR Delay  Hand  SES  MMSE  CDR  eTIV  nWBV  ASF\n",
      "0   28     5      1        15     0    2    18    0   284    20    0\n",
      "1   15     3      0         0     0    0    11    1   231    73   51\n",
      "2   16     3      1        32     0    0    16    1   254    51   31\n",
      "3   20     3      2       185     0    0    10    1   238    40   45\n",
      "4   28     9      0         0     0    3    16    0    16    48  247\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad125d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372,)\n",
      "0    Nondemented\n",
      "1       Demented\n",
      "2       Demented\n",
      "3       Demented\n",
      "4    Nondemented\n",
      "Name: Group, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6844da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 1)\n",
      "imbalance of this predicted variable is: 129.0% \n",
      "\n",
      "Imbalance Variables\n",
      "Nondemented    189\n",
      "Demented       146\n",
      "Converted       37\n",
      "Name: Group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def output_stats(y):\n",
    "    y = pd.DataFrame(y)\n",
    "    print (y.shape)\n",
    "    \n",
    "    y.columns = y.columns.str.strip() #to remove white spaces from column headers\n",
    "\n",
    "    imbalance = y.Group.value_counts() #check dataset for imbalances\n",
    "    \n",
    "    #imbalanace is the distribution of the training outcome. If the dataset is bias with more outcomes, the model will be better at predicting that outcome\n",
    "    if(imbalance.shape[0] > 1): # condition prevents code from throwing error if the dataset has only on type of output (e.g., 0 for days without intrusion)\n",
    "        print(f'imbalance of this predicted variable is: {round(imbalance[0]/imbalance[1], 2) * 100}% \\n\\nImbalance Variables\\n{imbalance}')\n",
    "    return y\n",
    "\n",
    "y_stats = output_stats(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a4b99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 3)\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "y_encoded = encode_output(y)\n",
    "print(y_encoded.shape)\n",
    "print(y_encoded[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1cb17",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e8f5aaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extra_tree_classifier(threshold, model, X, df):\n",
    "\n",
    "    feat_importances = pd.Series(model.feature_importances_, index = X.columns)\n",
    "    print(feat_importances)\n",
    "    correlations = feat_importances.nlargest(df.shape[1])\n",
    "\n",
    "    df = pd.DataFrame(correlations).reset_index()\n",
    "    df.columns = ['features', 'data']\n",
    "    \n",
    "    df_output = df[df['data'] >= threshold]\n",
    "    \n",
    "    #This is to ensure the feature selector returns features when feature correlation between X and y is 0 \n",
    "    while df_output.shape[0] < 3: # features must be a minimum of 3\n",
    "        \n",
    "        print(f'Current Feature size before adjustment: {df_output.shape[0]}')\n",
    "        threshold = round(threshold - 0.01, 2) # reduced threshold by 1% to increase number of threshold\n",
    "        print(f'New Threshold: {threshold}')\n",
    "        df_output = df[df['data'] >= threshold]\n",
    "        print(f'Current Feature size after adjustment: {df_output.shape[0]}')\n",
    "        if(threshold == 0):\n",
    "            print(f'Final Correlation: {threshold * 100}%')\n",
    "            break\n",
    "                \n",
    "    tooltip = df.data\n",
    "    tt = [str(round((ttip * 100), 2)) + '% correlation' for ttip in tooltip.values]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(9.5,5.5))\n",
    "    \n",
    "    ax.bar(df['features'], df['data'], align=\"center\")\n",
    "    cursor = mplcursors.cursor(hover=mplcursors.HoverMode.Transient)\n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        sel.annotation.set(text=tt[sel.target.index])\n",
    "        \n",
    "    plt.title('Feature Correlation with Output - Label\\n', loc='left', fontsize=22)\n",
    "    plt.xlabel('Features')\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.xticks(rotation = 90, fontsize=8)\n",
    "    plt.ylabel('Correlations')\n",
    "    plt.tight_layout()\n",
    "    plt.margins(x=0)\n",
    "    plt.show()\n",
    "       \n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db8538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age         0.044494\n",
      "EDUC        0.047345\n",
      "Visit       0.021904\n",
      "MR Delay    0.045420\n",
      "Hand        0.000000\n",
      "SES         0.051884\n",
      "MMSE        0.156185\n",
      "CDR         0.476652\n",
      "eTIV        0.046905\n",
      "nWBV        0.061865\n",
      "ASF         0.047346\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 684x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_features = extra_tree_classifier(0.04, model, X, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4219f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_input(data, alt_time_steps):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values\n",
    "    \n",
    "    # Reshape the data into a 3D array\n",
    "    num_samples = data.shape[0] \n",
    "    num_features = data.shape[1]\n",
    "    input = np.zeros((num_samples, alt_time_steps, num_features))\n",
    "    for i in range(num_samples):\n",
    "        if i+alt_time_steps < num_samples:\n",
    "            input[i, :, :] = data[i:i+alt_time_steps, :]\n",
    "            \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ce1c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP network for tabular data\n",
    "class classifier():\n",
    "    def __init__(self, num_tabular_features, desired_image_shape, time_step):\n",
    "        self.num_tabular_features = num_tabular_features\n",
    "        self.desired_image_shape = desired_image_shape\n",
    "        self.num_features = num_tabular_features\n",
    "        self.time_step = time_step\n",
    "        self.tabular_model = None\n",
    "        self.image_model = None\n",
    "        self.multi_modal_model = None\n",
    "    \n",
    "    def tabular_classifier(self):\n",
    "        tabular_model = Sequential(name=\"tabular_classifier\")\n",
    "        tabular_model.add(LSTM(self.num_features, return_sequences=True, input_shape=(self.time_step, self.num_features), activation='tanh', dropout=0.2))\n",
    "        tabular_model.add(LSTM((self.num_features*2), return_sequences=True, input_shape=(self.time_step, self.num_features), activation='sigmoid', dropout=0.2))\n",
    "        tabular_model.add(LSTM((self.num_features*4), input_shape=(self.time_step, self.num_features), activation='relu', dropout=0.2))\n",
    "        tabular_model.add(Dense(3, activation='softmax'))\n",
    "        adam = Adam(learning_rate=0.001)\n",
    "        tabular_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        \n",
    "        return tabular_model\n",
    "\n",
    "    # Define the CNN network for image data\n",
    "    def image_classifier(self):\n",
    "        image_input = Input(shape=self.desired_image_shape)\n",
    "        image_conv1 = Conv2D(32, (2, 2), activation='relu', padding='same')(image_input)\n",
    "        image_maxpool1 = MaxPooling2D((2, 2), padding='same')(image_conv1)\n",
    "        image_conv2 = Conv2D(64, (2, 2), activation='relu', padding='same')(image_maxpool1)\n",
    "        image_maxpool2 = MaxPooling2D((2, 2), padding='same')(image_conv2)\n",
    "        image_conv3 = Conv2D(128, (2, 2), activation='relu', padding='same')(image_maxpool2)\n",
    "        image_maxpool3 = MaxPooling2D((2, 2), padding='same')(image_conv3)\n",
    "        image_flat = Flatten()(image_maxpool2)\n",
    "#         image_LSTM = LSTM(128, activation='softmax')(image_flat)\n",
    "        image_output = Dense(3, activation='softmax')(image_flat)\n",
    "        image_model = Model(inputs=image_input, outputs=image_output, name=\"image_classifier\")\n",
    "        image_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return image_model\n",
    "      \n",
    "    def build_classifier(self):\n",
    "        self.tabular_model = self.tabular_classifier()\n",
    "        self.image_model = self.image_classifier()\n",
    "\n",
    "        # Combine the feature vectors using concatenation\n",
    "        combined_features = Concatenate()([self.tabular_model.output, self.image_model.output])\n",
    "\n",
    "        # Fusion Layer: You can add additional layers here if needed\n",
    "        fusion_layer = Dense(64, activation='relu')(combined_features)\n",
    "\n",
    "        # Output Layer for binary classification\n",
    "        output_layer = Dense(3, activation='sigmoid')(fusion_layer)\n",
    "\n",
    "        # Create the siamese network\n",
    "        self.multi_modal_model = Model(inputs=[self.tabular_model.input, self.image_model.input], outputs=output_layer, name=\"alheimers_classifier\")\n",
    "\n",
    "        # Compile the model\n",
    "        self.multi_modal_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77353aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder_path):\n",
    "    images_dict = {}\n",
    "    desired_size = (200, 200, 3)\n",
    "\n",
    "    for root, dirs, files in tqdm(os.walk(folder_path), desc=\"Processing\"):\n",
    "#         print(root)\n",
    "        # Skip the first entry (root is empty)\n",
    "        if not files:\n",
    "            continue\n",
    "        \n",
    "        # Sort the directories alphabetically\n",
    "        dirs.sort()\n",
    "\n",
    "        patient_images = []\n",
    "        for file in sorted(files):  # Sort files alphabetically\n",
    "            if file.endswith('.png') and \"nifti\" in file:\n",
    "                # get image path\n",
    "                image_path = os.path.join(root, file)\n",
    "\n",
    "                # Load the header and image data\n",
    "                image = keras.preprocessing.image.load_img(image_path, target_size=desired_size[:-1])\n",
    "                image = keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "                # Convert the image to RGB format\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Normalize the image to the range [0, 1]\n",
    "                image = image.astype('float32') / 255.0\n",
    "\n",
    "                # Append the preprocessed image to the list\n",
    "                patient_images.append(image)\n",
    "\n",
    "        # Store images in a dictionary with the folder as the key\n",
    "        images_dict[root] = patient_images\n",
    "\n",
    "    # Convert the dictionary values to a NumPy array\n",
    "    images_list = list(images_dict.values())\n",
    "    images = np.array(images_list, dtype=object)\n",
    "\n",
    "    return images, desired_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59fe710d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 373it [00:02, 128.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(372,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = r'D:/DATASET/Dimentia/Output'\n",
    "images, desired_size = load_images(image_path)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a2a37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372, 200, 200, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_prep = []\n",
    "for i in range(len(images)):\n",
    "    images_prep.append(images[i][0])\n",
    "images_prep = np.array(images_prep)\n",
    "images_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c197258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 4, 9)\n",
      "(372, 3)\n"
     ]
    }
   ],
   "source": [
    "time_step = 4\n",
    "X = X[selected_features['features']]\n",
    "X_reshaped = reshape_input(X, time_step)\n",
    "print(X_reshaped.shape)\n",
    "print(y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c95616e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         CDR\n",
       "1        MMSE\n",
       "2        nWBV\n",
       "3         SES\n",
       "4         ASF\n",
       "5        EDUC\n",
       "6        eTIV\n",
       "7    MR Delay\n",
       "8         Age\n",
       "Name: features, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1a3859a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 4, 9)              684       \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 4, 18)             2016      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 36)                7920      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 111       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,731\n",
      "Trainable params: 10,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 200, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 200, 200, 32)      416       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 100, 100, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 100, 100, 64)      8256      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 50, 50, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160000)            0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 480003    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 488,675\n",
      "Trainable params: 488,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 200, 200, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 200, 200, 32  416         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lstm_input (InputLayer)        [(None, 4, 9)]       0           []                               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 100, 100, 32  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 4, 9)         684         ['lstm_input[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 100, 100, 64  8256        ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 4, 18)        2016        ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 50, 50, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 36)           7920        ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 160000)       0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            111         ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            480003      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 6)            0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           448         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3)            195         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 500,049\n",
      "Trainable params: 500,049\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "al_classifier = classifier(len(selected_features), desired_size, time_step)\n",
    "al_classifier.build_classifier()\n",
    "al_classifier.tabular_model.summary()\n",
    "al_classifier.image_model.summary()\n",
    "al_classifier.multi_modal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2655a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tabular_train shape: (297, 4, 9)\n",
      "X_tabular_test shape: (75, 4, 9)\n",
      "X_images_train shape: (297, 200, 200, 3)\n",
      "X_images_test shape: (75, 200, 200, 3)\n",
      "y_train shape: (297, 3)\n",
      "y_test shape: (75, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_tabular, X_images, and y_reshaped are NumPy arrays\n",
    "\n",
    "# Split the data into training and testing sets while maintaining the correspondence\n",
    "X_tabular_train, X_tabular_test, X_images_train, X_images_test, y_train, y_test = train_test_split(\n",
    "    X_reshaped, images_prep, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the shapes of the resulting arrays\n",
    "print(\"X_tabular_train shape:\", X_tabular_train.shape)\n",
    "print(\"X_tabular_test shape:\", X_tabular_test.shape)\n",
    "print(\"X_images_train shape:\", X_images_train.shape)\n",
    "print(\"X_images_test shape:\", X_images_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4340566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 10s 635ms/step - loss: 1.0601 - accuracy: 0.5387 - val_loss: 1.0636 - val_accuracy: 0.3867\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 5s 544ms/step - loss: 1.0094 - accuracy: 0.5387 - val_loss: 1.0446 - val_accuracy: 0.3867\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 5s 544ms/step - loss: 0.9752 - accuracy: 0.5387 - val_loss: 1.0412 - val_accuracy: 0.3867\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 6s 557ms/step - loss: 0.9504 - accuracy: 0.5387 - val_loss: 1.0456 - val_accuracy: 0.3867\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.9334 - accuracy: 0.5387 - val_loss: 1.0546 - val_accuracy: 0.3867\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 0.9235 - accuracy: 0.5387 - val_loss: 1.0600 - val_accuracy: 0.3867\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 6s 560ms/step - loss: 0.9176 - accuracy: 0.5387 - val_loss: 1.0584 - val_accuracy: 0.3867\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 6s 608ms/step - loss: 0.9147 - accuracy: 0.5387 - val_loss: 1.0660 - val_accuracy: 0.3867\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 6s 595ms/step - loss: 0.9120 - accuracy: 0.5387 - val_loss: 1.0675 - val_accuracy: 0.3867\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.9106 - accuracy: 0.5387 - val_loss: 1.0648 - val_accuracy: 0.3867\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.9104 - accuracy: 0.5387 - val_loss: 1.0707 - val_accuracy: 0.3867\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 6s 640ms/step - loss: 0.9109 - accuracy: 0.5387 - val_loss: 1.0672 - val_accuracy: 0.3867\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.9095 - accuracy: 0.5387 - val_loss: 1.0740 - val_accuracy: 0.3867\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.9095 - accuracy: 0.5387 - val_loss: 1.0800 - val_accuracy: 0.3867\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.9095 - accuracy: 0.5387 - val_loss: 1.0812 - val_accuracy: 0.3867\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 6s 580ms/step - loss: 0.9092 - accuracy: 0.5387 - val_loss: 1.0772 - val_accuracy: 0.3867\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 6s 583ms/step - loss: 0.9096 - accuracy: 0.5387 - val_loss: 1.0729 - val_accuracy: 0.3867\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.9100 - accuracy: 0.5387 - val_loss: 1.0745 - val_accuracy: 0.3867\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.9104 - accuracy: 0.5387 - val_loss: 1.0735 - val_accuracy: 0.3867\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 5s 547ms/step - loss: 0.9102 - accuracy: 0.5387 - val_loss: 1.0820 - val_accuracy: 0.3867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cdcdc38790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "al_classifier.multi_modal_model.fit([X_tabular_train, X_images_train], \n",
    "          y_train,\n",
    "          verbose=1,\n",
    "          use_multiprocessing=True,\n",
    "          workers=4,\n",
    "          validation_data=([X_tabular_test, X_images_test], y_test), \n",
    "          epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
